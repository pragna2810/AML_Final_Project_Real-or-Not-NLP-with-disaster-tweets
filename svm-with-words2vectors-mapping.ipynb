{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/holygrail/submission.csv\n",
      "/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\n",
      "/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from nltk.stem import PorterStemmer #normalize word form\n",
    "from nltk.probability import FreqDist #frequency word count\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords #stop words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The functions used for text preprosessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hyperlink\n",
    "\n",
    "def text_cleaning_hyperlink(text):\n",
    "    \n",
    "    return re.sub(r\"http\\S+\",\"\",text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation marks\n",
    "\n",
    "def text_cleaning_punctuation(text):\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    \n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean stopwords\n",
    "\n",
    "def text_cleaning_stopwords(text):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    word_token = word_tokenize(text)\n",
    "    \n",
    "    filtered_sentence = [w for w in word_token if not w in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_sentence) #return string of no stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all letters into lowercase ones\n",
    "\n",
    "def text_cleaning_lowercase(text):\n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extract(text_lst):\n",
    "    txt = []\n",
    "    for i,x in enumerate(text_lst):\n",
    "        \n",
    "        for j,p in enumerate(x):\n",
    "            \n",
    "            txt.append(p)\n",
    "    \n",
    "    return txt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove digits from the text\n",
    "\n",
    "def remove_digits(txt):\n",
    "    \n",
    "    no_digits = ''.join(i for i in txt if not i.isdigit())\n",
    "    \n",
    "    return no_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we preprocess the TRAINING data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we clean the keywords for the TRAINING data\n",
    "\n",
    "train.keyword = train.keyword.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x.lower())))) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we clean the text for the TRAINING data\n",
    "\n",
    "train.text = train.text.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x.lower())))))\n",
    "train.text = train.text.apply(lambda x: list(set(x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add keywords to the text of the TRAINING data\n",
    "# train['k_t'] = train.apply(lambda x : x['text'] + [x['keyword']] if type(x['keyword']) == str else x['text'],axis=1) #add keyword to text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add keywords to the text of the TRAINING data\n",
    "train['k_t'] = train.apply(lambda x : x['text'] + [x['keyword']] if type(x['keyword']) == str else x['text'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the text of the TRAINING set\n",
    "\n",
    "ps_1 = PorterStemmer()\n",
    "wnl_1 = WordNetLemmatizer()\n",
    "text_reconstruct = []\n",
    "\n",
    "for i,x in enumerate(train.k_t.values):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        a = wnl_1.lemmatize(ps_1.stem(x))\n",
    "\n",
    "        \n",
    "    except AttributeError:\n",
    "        \n",
    "        a = list(set([wnl_1.lemmatize(ps_1.stem(word)) for j,word in enumerate(x)]))\n",
    "        \n",
    "    \n",
    "    text_reconstruct.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the keywords to the text of the TRAINING set\n",
    "\n",
    "train.k_t = text_reconstruct\n",
    "train_word = train.k_t.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then we preprocess the TEST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean keywords of the TEST data\n",
    "\n",
    "test.keyword = test.keyword.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x.lower())))) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text of the TEST data\n",
    "\n",
    "test.text = test.text.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x.lower())))))\n",
    "test.text = test.text.apply(lambda x: list(set(x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add (weighted) keywords to the text of the TEST data\n",
    "\n",
    "test['k_t'] = test.apply(lambda x : x['text'] + [x['keyword']+x['keyword']+x['keyword']] if type(x['keyword']) == str else x['text'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the text of the TEST set\n",
    "\n",
    "ps_2 = PorterStemmer()\n",
    "wnl_2 = WordNetLemmatizer()\n",
    "text_reconstruct_test = []\n",
    "\n",
    "for i,x in enumerate(test.k_t.values):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        a = wnl_2.lemmatize(ps_2.stem(x))\n",
    "\n",
    "        \n",
    "    except AttributeError:\n",
    "        \n",
    "        a = list(set([wnl_2.lemmatize(ps_2.stem(word)) for j,word in enumerate(x)]))\n",
    "        \n",
    "    \n",
    "    text_reconstruct_test.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the keywords to the text of the TEST set\n",
    "\n",
    "test.k_t = text_reconstruct_test\n",
    "test_word = test.k_t.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we map the tokenized data to vectors (to capture some semantic similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "path_for_word2vec = \"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(path_for_word2vec, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to map words to vectors\n",
    "\n",
    "def average_w2v(list_of_tokens, vct, generate_missing = False, dimentions = 300):\n",
    "    if len(list_of_tokens) < 1:\n",
    "        return np.zeros(dimentions)\n",
    "    if generate_missing:\n",
    "        vector = [vct[item] if item in vct else np.random.rand(dimentions) for item in list_of_tokens]\n",
    "    else:\n",
    "        vector = [vct[item] if item in vct else np.zeros(dimentions) for item in list_of_tokens]\n",
    "    total_length = len(vector)\n",
    "    sum_of_vectors = np.sum(vector, axis=0)\n",
    "    average = np.divide(sum_of_vectors, total_length)\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map words to vectors (using the googlenewsvectorsnegative300 database)\n",
    "\n",
    "def word2vec_mapping(vect, our_word, generate_missing = False):\n",
    "    mapping = our_word.apply(lambda x: average_w2v(x, vect, generate_missing = generate_missing))\n",
    "    return list(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here The SVM + Word2Vec Model is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "our_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_input_train = train_word.apply(our_tokenizer.tokenize)  # tokenize the TRAINING set\n",
    "tokenized_input_test = test_word.apply(our_tokenizer.tokenize)    # tokenize the TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_train = word2vec_mapping(word2vec, train.text) # vectorize the TRAINING set\n",
    "mapped_test = word2vec_mapping(word2vec,test.text)    # vectorize the TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_SVM = SVC(C = 2)\n",
    "classifier_SVM.fit(mapped_train, train.target)\n",
    "y_predicted_SVM = classifier_SVM.predict(mapped_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostPreditiction corrections that take into accout important keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna('Zilch')\n",
    "important_key_words = train.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'count', 'target':'frequency'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_list = ['bushfires','evacuated','forestfire','hostages','rescuers','sinkhole','thunderstorm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_disaster = 0.95\n",
    "keyword_list_disaster95 = list(important_key_words[important_key_words['frequency']>prob_disaster].index) + additional_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_of_95certain_disasters = test['id'][test.keyword.isin(keyword_list_disaster95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.zeros(len(y_predicted_SVM))\n",
    "\n",
    "for i in range(0,len(y_predicted_SVM)):\n",
    "    if i in numbers_of_95certain_disasters:\n",
    "        y_predicted[i] = 1\n",
    "    else:\n",
    "        y_predicted[i] = y_predicted_SVM[i]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"target\"] = [int(i) for i in y_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
